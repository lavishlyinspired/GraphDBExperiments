# NLP Integration for Biomedical Literature Processing

## üåü Overview

The ETL pipeline now includes **Natural Language Processing (NLP)** capabilities to extract medical entities from biomedical literature and link them to the structured ontology. This integration combines features from **folder 2 (gcp_nlp)** with the ontology-driven knowledge graph.

## üéØ Key Capabilities

### 1. **Literature Ingestion**
- Load biomedical case reports and research articles
- Process from CSV format with: URI, Title, Body, Publication Date
- Example: PubMed Central (PMC) articles on lung cancer

### 2. **Entity Extraction**
- **Deterministic Pattern Matching**: Rule-based extraction (no API required)
- **Optional GCP NLP API**: Google Cloud Natural Language API integration
- Extracted entities: Histology, Stage, Biomarker, Mutation, Drug, Therapy, Test, Outcome

### 3. **Semantic Linking**
- Automatically link articles to ontology concepts
- Create `refersTo` relationships between articles and entities
- Support graph-based literature exploration

## üìä Extracted Entity Types

| Entity Type | Examples | Pattern |
|-------------|----------|---------|
| **Histology** | adenocarcinoma, squamous cell carcinoma, SCLC, NSCLC | Tumor subtypes |
| **Stage** | Stage IIIA, T2N1M0 | Cancer staging |
| **Biomarker** | EGFR, ALK, KRAS, PD-L1 | Genetic markers |
| **Mutation** | L858R, Ex19del, G12C | Gene mutations |
| **Drug** | osimertinib, pembrolizumab, cisplatin | Medications |
| **Therapy** | chemotherapy, immunotherapy, surgery, VATS | Treatments |
| **Test** | CT scan, PET-CT, genomic test, NGS | Diagnostics |
| **Outcome** | complete response, progression, metastasis | Results |

## üöÄ Usage

### Basic Usage (No API Required)

```python
python lung_cancer_etl_engine.py
```

The ETL automatically:
1. Loads articles from `LungcancerArticle.csv`
2. Extracts entities using pattern matching
3. Links articles to ontology concepts
4. Generates RDF triples

### Output Example

```turtle
<http://lungkg.org/resource/Article_PMC9392234> a ont:Article ;
    rdfs:label "Triple primary lung cancer: a case report" ;
    ont:title "Triple primary lung cancer: a case report" ;
    ont:body "This report presents a 76-year-old man..." ;
    ont:publicationDate "2022-08-19"^^xsd:date ;
    ont:about ont:LungCancer ;
    ont:refersTo res:Histology_Adenocarcinoma,
                 res:Histology_Squamous_Cell_Carcinoma,
                 res:Histology_Small_Cell_Lung_Cancer,
                 res:Therapy_Chemotherapy,
                 res:Therapy_Surgery,
                 res:Biomarker_EGFR .
```

### Advanced: GCP NLP API (Optional)

```python
from nlp_processor import process_article_text

# With GCP NLP API (requires google-cloud-language package)
result = process_article_text(
    article_body=text,
    ontology_graph=g,
    ont_ns=ONT,
    res_ns=RES,
    use_gcp=True,
    api_key="YOUR_GCP_API_KEY"
)

print(f"Entities found: {result['entity_count']}")
print(f"NLP API used: {result['nlp_used']}")
```

## üìÇ File Structure

```
5.Python_ontodriven_kgraph/
‚îú‚îÄ‚îÄ lung_cancer_etl_engine.py        # Main ETL (now with NLP)
‚îú‚îÄ‚îÄ nlp_processor.py                 # NEW: NLP extraction module
‚îú‚îÄ‚îÄ ttl_shacl_data/
‚îÇ   ‚îú‚îÄ‚îÄ lung_cancer_kg_schema.ttl    # Updated schema with Article class
‚îÇ   ‚îú‚îÄ‚îÄ mapping_config.json          # Updated with articles section
‚îÇ   ‚îú‚îÄ‚îÄ LungcancerArticle.csv        # NEW: Biomedical articles data
‚îÇ   ‚îî‚îÄ‚îÄ [patient CSV files]
‚îî‚îÄ‚îÄ ouput/
    ‚îî‚îÄ‚îÄ lung_cancer_instances_out.ttl # Generated with article entities
```

## üî¨ NLP Processing Pipeline

### Step 1: Pattern-Based Extraction

The `EntityExtractor` class uses regex patterns to identify medical terms:

```python
'Biomarker': [
    r'\b(EGFR|ALK|ROS1|BRAF|KRAS|MET|RET|NTRK|PD-L1|HER2)\b'
],
'Mutation': [
    r'\b(L858R|Ex19del|T790M|G12C|V600E|EML4-ALK)\b'
]
```

### Step 2: Entity Canonicalization

Extracted text is normalized to canonical URIs:
- `"lung cancer"` ‚Üí `res:Disease_Lung_Cancer`
- `"EGFR mutation"` ‚Üí `res:Biomarker_EGFR`
- `"Stage IIIA"` ‚Üí `res:Stage_IIIA`

### Step 3: Triple Generation

For each extracted entity:
1. Create entity URI
2. Add `rdf:type` assertion
3. Add `rdfs:label`
4. Link article with `refersTo` predicate

## üåê Neo4j Integration

### Import Articles with Entities

```cypher
// Import schema and instances (includes articles)
CALL n10s.rdf.import.fetch(
  "file:///path/to/lung_cancer_kg_schema.ttl",
  "Turtle"
);

CALL n10s.rdf.import.fetch(
  "file:///path/to/lung_cancer_instances_out.ttl",
  "Turtle"
);

// Verify article entity extraction
MATCH (a:ns0__Article)-[:ns0__refersTo]->(e)
RETURN a.ns0__title AS Article,
       labels(e) AS EntityType,
       e.rdfs__label AS Entity
LIMIT 20;
```

### Explore Literature-Concept Connections

```cypher
// Find articles mentioning specific biomarkers
MATCH (a:ns0__Article)-[:ns0__refersTo]->(b:ns0__Biomarker)
WHERE b.rdfs__label CONTAINS 'EGFR'
RETURN a.ns0__title, a.ns0__publicationDate
ORDER BY a.ns0__publicationDate DESC;

// Find articles discussing specific treatments
MATCH (a:ns0__Article)-[:ns0__refersTo]->(t:ns0__Therapy)
WHERE t.rdfs__label CONTAINS 'Immunotherapy'
RETURN a.ns0__title, a.ns0__uri;

// Connect patient data to research literature
MATCH (p:ns0__Patient)-[:ns0__hasTumor]->(tumor)-[:ns0__hasHistology]->(h:ns0__Histology)
MATCH (a:ns0__Article)-[:ns0__refersTo]->(h)
RETURN p.ns0__patientId AS Patient,
       h.rdfs__label AS Histology,
       collect(a.ns0__title)[..3] AS RelevantArticles;
```

## üìà Example Query Results

### Articles by Entity Type

```cypher
MATCH (a:ns0__Article)-[:ns0__refersTo]->(e)
RETURN labels(e)[0] AS EntityType, count(a) AS ArticleCount
ORDER BY ArticleCount DESC;
```

**Results:**
```
EntityType       ArticleCount
Histology        10
Therapy          8
Outcome          5
Biomarker        3
Stage            2
```

### Most Mentioned Entities

```cypher
MATCH (a:ns0__Article)-[:ns0__refersTo]->(e)
RETURN e.rdfs__label AS Entity,
       count(a) AS Mentions
ORDER BY Mentions DESC
LIMIT 10;
```

## üîß Configuration

### Enable/Disable NLP for Sections

In `mapping_config.json`:

```json
"articles": {
  "file": "ttl_shacl_data/LungcancerArticle.csv",
  "subject": "Article_{uri}",
  "type": "Article",
  "nlp_extraction": true,  // ‚Üê Enable NLP
  "datatype_props": {
    "uri": "uri",
    "title": "title",
    "body": "body",
    "publicationDate": "date"
  }
}
```

### Customize Entity Patterns

Edit `nlp_processor.py`:

```python
self.concept_patterns = {
    'Biomarker': [
        r'\b(EGFR|ALK|YOUR_CUSTOM_PATTERN)\b',
    ],
    # Add more patterns...
}
```

## üéì Use Cases

### 1. **Literature Review**
Find all articles discussing a specific treatment approach:

```cypher
MATCH (a:ns0__Article)-[:ns0__refersTo]->(t:ns0__Therapy)
WHERE t.rdfs__label =~ '.*Targeted.*'
RETURN a.ns0__title, a.ns0__uri;
```

### 2. **Evidence-Based Treatment**
Link patient biomarkers to research evidence:

```cypher
MATCH (p:ns0__Patient)-[:ns0__testedForBiomarker]->(b:ns0__Biomarker)
MATCH (a:ns0__Article)-[:ns0__refersTo]->(b)
RETURN p.ns0__patientId, b.rdfs__label,
       collect(a.ns0__title) AS RelevantStudies;
```

### 3. **Trend Analysis**
Track mention of therapies over time:

```cypher
MATCH (a:ns0__Article)-[:ns0__refersTo]->(t:ns0__Therapy)
WHERE t.rdfs__label CONTAINS 'Immunotherapy'
RETURN a.ns0__publicationDate.year AS Year, count(a) AS Articles
ORDER BY Year;
```

## ‚öôÔ∏è Advanced: GCP NLP API Setup

### Prerequisites

```bash
pip install google-cloud-language
```

### Configure API Key

Set environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
```

Or pass API key directly in code.

### Benefits of GCP NLP

- More comprehensive entity extraction
- Sentiment analysis
- Entity salience (importance) scoring
- Multi-language support
- Recognition of person names, locations, organizations

## üìä Performance

### Deterministic Extraction (Default)
- **Speed**: ~0.1 seconds per article
- **Cost**: Free
- **Accuracy**: High for medical terms (pattern-based)

### GCP NLP API
- **Speed**: ~0.5 seconds per article
- **Cost**: ~$1 per 1,000 articles
- **Accuracy**: Higher recall for general entities

## üîç Troubleshooting

### Issue: No entities extracted

**Cause**: Article text too short or no matching patterns

**Solution**: Check `nlp_processor.py` patterns match your domain

### Issue: GCP API errors

**Cause**: Missing credentials or quota exceeded

**Solution**: Verify `GOOGLE_APPLICATION_CREDENTIALS` and check quota

### Issue: URI encoding errors

**Cause**: Special characters in article URIs

**Solution**: URIs are automatically URL-encoded in mapping

## üìö Related Documentation

- [README.md](README.md) - Main project overview
- [RELATIONSHIPS_SUMMARY.md](RELATIONSHIPS_SUMMARY.md) - All 16 relationships (13 + 3 new)
- [SPARQL_CYPHER_QUERIES.md](SPARQL_CYPHER_QUERIES.md) - Query examples
- [NEO4J_SHACL_VALIDATION.md](NEO4J_SHACL_VALIDATION.md) - Validation guide

## üéâ Summary

The NLP integration enables:
- ‚úÖ **Automated** entity extraction from biomedical text
- ‚úÖ **Semantic linking** of literature to structured data
- ‚úÖ **Graph exploration** across evidence and patient data
- ‚úÖ **Flexible** pattern-based or AI-powered extraction
- ‚úÖ **Scalable** processing of large document collections

**New Relationships:**
- `refersTo` (Article ‚Üí Entity)
- `mentions` (Article ‚Üí NamedEntity)
- `about` (Article ‚Üí Topic)

**Total: 16 relationships** in the unified knowledge graph!
